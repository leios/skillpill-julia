%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Modified by Jeremie Gillet in November 2015 to make an OIST Skill Pill template
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

\usetheme{Madrid}

\definecolor{OISTcolor}{rgb}{0.65,0.16,0.16}
\usecolortheme[named=OISTcolor]{structure}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\setbeamertemplate{itemize items}[default]
\setbeamertemplate{enumerate items}[default]
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{textpos} % Use for positioning the Skill Pill logo
\usepackage{fancyvrb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand*{\lstitem}[1]{
  \setbox0\hbox{\lstinline{#1}}  
  \item[\usebox0]  
  % \item[\hbox{\lstinline{#1}}]
  \hfill \\
}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Skill Pill]{Skill Pill: Julia} % The short title appears at the bottom of every slide, the full title is only on the title page
\subtitle{Lecture 4: Distributed and parallel computing}

\author{Valentin Churavy \and James Schloss} % Your name
\institute[OIST] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Okinawa Institute of Science and Technology \\ % Your institution for the title page
\textit{valentin.churavy@oist.jp} \\
\textit{james.schloss@oist.jp} % Your email address
}
\date{July 13, 2017} % Date, can be changed to a custom date

\begin{document}

\setbeamertemplate{background}{\includegraphics[width=\paperwidth]{SPbackground.png}} % Adding the background logo

\begin{frame}
\vspace*{1.4cm}
\titlepage % Print the title page as the first slide
\end{frame}

\setbeamertemplate{background}{} % No background logo after title frame

\addtobeamertemplate{frametitle}{}{% Adding the Skill Pill logo on the title screen after title frame
\begin{textblock*}{100mm}(.92\textwidth,-0.9cm)
\includegraphics[height=0.85cm]{julia.pdf}
\end{textblock*}}

\begin{frame}
  \tableofcontents
\end{frame}

\section{Levels of parallelism}
\begin{frame}{Introduction}
  \pause
  \begin{block}{Necessary packages}
    \begin{itemize}
      \item SIMD.jl
      \item MPI.jl
      \item DistributedArrays.jl
      \item CUDAnative.jl if your computer has a NVidia GPU
    \end{itemize}
  \end{block}
\end{frame}
\begin{frame}{Levels of parallelism}
  \begin{itemize}
    \item Instruction level parallelism
    \item Shared-memory and threading
    \item Distributed
    \item Accelerators e.g.GPGPU
  \end{itemize}
\end{frame}

\section{Instruction level parallelism}
\begin{frame}[fragile]{What is instruction level parallelism}

  \begin{lstlisting}
  function padd(a, b, x, y)
    c = a + b
    z = x + y
    return c, z
  end
  \end{lstlisting}

  \begin{block}{Observation}
     The two operations are independent of each other and we could execute them in parallel.
  \end{block}

  \begin{enumerate}
    \item Use \lstinline{@code_llvm} and \lstinline{@code_native} to understand what is happening
    \item Establish a baseline performance with \lstinline{@benchmark}
    \item Start Julia with \lstinline{julia -O3}
    \item Compare the llvm and native code and your benchmark results
    \item Note that there is next to no performance benefit in this example, but that changes once you scale up
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]{SIMD and loops}
  \begin{lstlisting}
  function add(out, x, y)
    for i in 1:length(out)
      out[i] = x[i] + y[i]
    end
  end
  \end{lstlisting}

  \begin{block}{Observation}
    Each loop iteration is independent.
  \end{block}
  \begin{enumerate}
    \item Learn about \lstinline{@inbounds}
  \end{enumerate}
\end{frame}
\begin{frame}[fragile]{Reductions and loop-dependencies}
  \begin{lstlisting}
  function sum(x)
    acc = 0.0
    for i in 1:length(x)
       acc += x[i]
    end
  end
  \end{lstlisting}
  \begin{block}{Observation}
    Is each loop iteration independent from each other? Yes and no.
    Standard addition is associative and the order of operations has no impact.
    Floating-point addition is non-associative and the order of operations is important.
    The compiler cannot vectorise this loop, without changing the semantics.
  \end{block}
  \begin{enumerate}
    \item Learn about \lstinline{@simd}
  \end{enumerate}
\end{frame}
\begin{frame}[fragile]{Explicit SIMD}
  \begin{block}{SIMD.jl}
    Instead of relying on the compiler to optimise and vectorise our code correctly we can also write explicit SIMD code.
  \end{block}
  \begin{lstlisting}
  using SIMD
  function add(out::Vector{Float64}, x::Vector{Float64}, y::Vector{Float64})
    # My laptop supports AVX 256bit 4xFloat64
    @assert length(x) % 4 == 0
    for i in 1:4:length(x)
      vx = vload(Vec{4, Float64}, x, i)
      vy = vload(Vec{4, Float64}, y, i)
      vo = vx + vy
      vstore(vo, out, i)
    end
  end
  \end{lstlisting}
\end{frame}
\section{Threading}
\begin{frame}{Fork-Join model}
  \begin{block}{Caveat}
    Julia threading model is based on a fork-join approach and is still considered experimental.
    The Julia developers are working with Intel on bringing a modern and full-fledged threading model to Julia for 1.0
  \end{block}
  \pause
  \begin{block}{What is fork-join}
    Fork-join describes the control flow that a group of threads undergo. Execution is forked across to all threads and at a later point execution is joined together. In Julia all threads have to execute the same lambda.
  \end{block}
\end{frame}
\begin{frame}[fragile]
  Start Julia with \lstinline{JULIA_NUM_THREADS=4 julia} the number of threads needs to be set before starting Julia.

  \begin{lstlisting}
  using Base.Threads

  a = zeros(nthreads()*10)
  @threads for i in 1:length(a)
    a[i] = threadid()
  end
  \end{lstlisting}
  \begin{block}{Caveats}
    Special care needs to be taken for access to global state, which includes IO and random numbers.
  \end{block}
\end{frame}
\begin{frame}[fragile]{Further examples}
  \begin{lstlisting}
  A = zeros(10_000)
  @threads for i in 1:length(A)
    A[i] = rand() # Note rand uses a global variable Base.GLOBAL_RNG
  end

  rngs = [MersenneTwister(rand(UInt64)) for i in 1:nthreads()]
  @threads for i in 1:length(A)
    A[i] = rand(rngs[threadid()])
  end
  \end{lstlisting}
\end{frame}
\begin{frame}[fragile]{Support for Atomics}
  \begin{lstlisting}
  acc = 0
  @threads for i in 1:10_000
    acc += 1
  end
  \end{lstlisting}
  \begin{block}{}
    To prevent the datarace we can use Atomics to ensure read-write consistency.
  \end{block}
  \begin{lstlisting}
  acc = Atomic{Int64}(0)
  @threads for i in 1:10_000
    atomic_add!(acc, 1)
  end
  \end{lstlisting}
\end{frame}
\section{Distributed}
\begin{frame}{MPI}
  \begin{block}{}
    Julia supports MPI through \lstinline{MPI.jl}. If you are comfortable with MPI you can use it with very low overhead.
  \end{block}
\end{frame}
\begin{frame}{Parallel programming}
  Starting Julia with \lstinline{julia -p 4} will create one master process and four worker processes. You can use remotecalls to execute work on a worker and to get back the results. Based on this \lstinline{DistributeArrays.jl} provides a powerful global array interface.
  \lstinline{@everywhere} will execute a piece of code on all processes.
\end{frame}
\section{CUDAnative}
\begin{frame}{CUDAnative}
  \begin{block}{CUDAnative}
    A recent development is the ability to write raw CUDA code in Julia! Learn more about it at \url{https://github.com/JuliaGPU/CUDAnative.jl}
  \end{block}
\end{frame}
\end{document}
